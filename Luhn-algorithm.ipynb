{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90f397b-20e9-44c2-9eaf-af4ebe8a9da9",
   "metadata": {},
   "source": [
    "# Luhn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2a495-a4b8-4b1d-a996-0e23caa4a8c6",
   "metadata": {},
   "source": [
    "Based on TF-IDF (TF — term frequency, IDF — inverse document frequency).\n",
    "\n",
    "Original [article](https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf)\n",
    "\n",
    "## Steps\n",
    "\n",
    "- Preprocess text\n",
    "- Calculate sentence score\n",
    "- Summarize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7d18d-51be-4061-ae35-1b9fce5751ae",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa1f2d3-3497-41b8-bf2e-ce4ac53ac381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb5bbf7-0525-44b2-814e-76e3235d545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mykolafant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mykolafant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6aa30b-ba19-47c5-9d10-17d24a08118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is human like intelligence machines. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior machines. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"\"\"Artificial intelligence is human like intelligence machines. \n",
    "                   It is the study of intelligent artificial agents. \n",
    "                   Science and engineering to produce intelligent machines. \n",
    "                   Solve problems and have intelligence. \n",
    "                   Related to intelligent behavior machines. \n",
    "                   Developing of reasoning machines. \n",
    "                   Learn from mistakes and successes. \n",
    "                   Artificial intelligence is related to reasoning in everyday situations.\"\"\"\n",
    "original_text = re.sub(r'\\s+', ' ', original_text)\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977d9ce3-b35e-49a5-8333-1a358423e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b61731b-5f9f-4d6b-82fb-55143ed7976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  formatted_text = text.lower()\n",
    "  tokens = []\n",
    "  for token in nltk.word_tokenize(formatted_text):\n",
    "    tokens.append(token)\n",
    "  tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "  formatted_text = ' '.join(element for element in tokens)\n",
    "\n",
    "  return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a09077-2396-49ff-80b9-ca5332840333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence human like intelligence machines study intelligent artificial agents science engineering produce intelligent machines solve problems intelligence related intelligent behavior machines developing reasoning machines learn mistakes successes artificial intelligence related reasoning everyday situations'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text = preprocess(original_text)\n",
    "formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4398f17-e840-4e07-b033-6c80adc24963",
   "metadata": {},
   "source": [
    "### Calculate sentence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "880a69e0-4ce9-4475-ab68-be01482dce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentences_score(sentences, important_words, distance):\n",
    "    scores = []\n",
    "    sentence_index = 0\n",
    "    for sentence in [nltk.word_tokenize(sentence) for sentence in sentences]:\n",
    "        word_index = []\n",
    "        for word in important_words:\n",
    "            try:\n",
    "                word_index.append(sentence.index(word))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        word_index.sort()\n",
    "        if (len(word_index) == 0):\n",
    "            continue\n",
    "\n",
    "        groups_list = []\n",
    "        group = [word_index[0]]\n",
    "        i = 1\n",
    "        while i < len(word_index):\n",
    "            if word_index[i] - word_index[i - 1] < distance:\n",
    "                group.append(word_index[i])\n",
    "            else:\n",
    "                groups_list.append(group[:])\n",
    "                group = [word_index[i]]\n",
    "            i += 1\n",
    "        groups_list.append(group)\n",
    "        max_group_score = 0\n",
    "        for g in groups_list:\n",
    "            important_words_in_group = len(g)\n",
    "            total_words_in_group = g[-1] - g[0] + 1\n",
    "            score = 1.0 * important_words_in_group**2/total_words_in_group\n",
    "\n",
    "            if score > max_group_score:\n",
    "                max_group_score = score\n",
    "        scores.append((max_group_score, sentence_index))\n",
    "        sentence_index += 1\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69da2bd-829a-4722-bb46-d9eddf539bb0",
   "metadata": {},
   "source": [
    "### Summarize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36fe3ee6-96d6-486f-93da-22353b321e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, top_n_words_count, distance):\n",
    "    original_sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
    "    formatted_sentences = [preprocess(original_sentence) for original_sentence in original_sentences]\n",
    "    words = [word for sentence in formatted_sentences for word in nltk.word_tokenize(sentence)]\n",
    "    frequency = nltk.FreqDist(words)\n",
    "    top_n_words = [word[0] for word in frequency.most_common(top_n_words_count)]\n",
    "    sentences_score = calculate_sentences_score(formatted_sentences, top_n_words, distance)\n",
    "    print(sentences_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d14367f5-7acc-4656-a808-0e6484aaef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 0), (2.0, 1), (2.0, 2), (1.0, 3), (2.0, 4), (1.0, 5), (3.0, 6)]\n"
     ]
    }
   ],
   "source": [
    "summarize(original_text, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88457e5c-4dc3-4516-84ba-a7300481c1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
